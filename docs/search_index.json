[
["diag2.html", "10 Diagnósticos parte II Matriz sombrero o hat ¿Qué es extrapolación oculta? Punto atípico y punto influyente Distancia de Cook DFFITS DFBETAS", " 10 Diagnósticos parte II En este capítulo se presentan otras herramientas útiles para realizar diagnósticos. Matriz sombrero o hat La matriz sombrero o matriz Hat se define así: \\[ \\boldsymbol{H} = \\boldsymbol{X}(\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top \\] Esta matriz contiene en su diagonal las distancias relativas desde el centroide de los datos hasta cada uno de los puntos. En la siguiente figura se ilustra el concepto de distancia relativa entre el centroide (color rojo) de las variables explicativas y cada uno de los puntos para un caso con tres variables explicativas \\(x_1\\), \\(x_2\\) y \\(x_3\\). La cantidad \\(h_{ii}\\) se llama leverage y corresponde al elemento \\(i\\) de la diagonal de la matriz sombrero \\(\\boldsymbol{H}\\). Los valores de \\(h_{ii}\\) siempre están entre 0 y 1. Si la observación \\(i\\)-ésima tiene un valor grande de \\(h_{ii}\\) significa que ella tiene valores inusuales de \\(\\boldsymbol{x}_i\\), mientras que valores pequeños de \\(h_{ii}\\) significa que la observación se encuentra cerca del centroide de los datos. La distancia \\(h_{ii}\\) no incluye información de la variable respuesta \\(y\\), solo de las covariables, esto se nota claramente en la fórmula de \\(\\boldsymbol{H}\\) dada arriba. ¿Para qué se usan los \\(h_{ii}\\) en la práctica? En el siguiente apartado se explicará el uso de los \\(h_{ii}\\). ¿Qué es extrapolación oculta? Suponga tenemos un modelo de regresión una variable respuesta y dos covariables \\(x_1\\) y \\(x_2\\). En la siguiente figura se ilustra los posibles datos desde una vista superior (sin ver los valores de \\(y\\)). Esa elipse o forma se llama Regressor Variable Hull (RVH) o cascarón de los datos. Una vez se tenga el modelo ajustado podríamos usar valores de \\(x_1\\) y \\(x_2\\) para estimar la media de \\(y\\). Lo ideal es usar el modelo para predecir la media de \\(y\\) con valores de \\(x_1\\) y \\(x_2\\) que se encuentren dentro del cascarón. Si tratamos de estimar la media de \\(y\\) para valores de las covariables fuera del cascarón, como en el caso del punto rojo, no podemos garantizar que el modelo tenga un buen desempeño debido a que el modelo no se entrenó con ese tipo de ejemplos. El problema de extrapolación oculta se presenta cuando tratamos de predecir información de \\(y\\) con covariables fuera del cascarón. La extrapolación oculta es fácil de identificarla cuando sólo se tiene dos covariables, pero, ¿cómo saber si se está haciendo extrapolación oculta cuando se tienen varias covariables. Supongamos que queremos saber si el vector de covariables \\(\\boldsymbol{x}_0=(x_1, x_2, \\ldots, x_p)^\\top\\) está o no dentro del cascarón, dicho de otra manera, ¿se cometería extrapolación oculta usando \\(\\boldsymbol{x}_0\\)?. Los pasos para determinar si \\(\\boldsymbol{x}_0\\) está o no dentro del cascarón son: Calcular la matriz \\(\\boldsymbol{H}\\). Obtener los valores \\(h_{ii}\\) de la matriz \\(\\boldsymbol{H}\\). Identificar \\(h_{max} = max\\{h_{11}, h_{22}, \\ldots, h_{nn}\\}\\). Calcular \\(h_{00} = \\boldsymbol{x}_0 (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{x}_0^\\top\\). Si \\(h_{00} &gt; h_{max}\\) el punto \\(\\boldsymbol{x}_0\\) está fuera del cascarón y se podría estár cometiendo extrapolación oculta. Los valores \\(h_{ii}\\) se pueden obtener al calcular la matriz \\(\\boldsymbol{H}\\). Otra forma de obtener los \\(h_{ii}\\) es ajustando el modelo de regresión y luego usando la función hatvalues(model) o lm.influence(model). Ejemplo Calcular los valores \\(h_{ii}\\) para un modelo de regresión y ~ x + z con los siguientes datos. y &lt;- c(2, 3, 6, 5) x &lt;- c(3, 5, 6, 7) z &lt;- c(5, 4, 6, 3) Solución A seguir se muestran las tres formas para obtener los valores \\(h_{ii}\\). # Forma 1 X &lt;- cbind(1, x, z) H &lt;- X %*% solve(t(X) %*% X) %*% t(X) H ## [,1] [,2] [,3] [,4] ## [1,] 8.333333e-01 3.333333e-01 -3.552714e-15 -0.1666667 ## [2,] 3.333333e-01 3.333333e-01 -9.992007e-16 0.3333333 ## [3,] -2.664535e-15 -1.110223e-15 1.000000e+00 0.0000000 ## [4,] -1.666667e-01 3.333333e-01 -1.332268e-15 0.8333333 diag(H) ## [1] 0.8333333 0.3333333 1.0000000 0.8333333 # Forma 2 mod &lt;- lm(y ~ x + z) hatvalues(mod) ## 1 2 3 4 ## 0.8333333 0.3333333 1.0000000 0.8333333 # Forma 3 lm.influence(mod)$hat ## 1 2 3 4 ## 0.8333333 0.3333333 1.0000000 0.8333333 Reto para el lector Use la información del ejemplo anterior y determine si la observación con valores de \\(x=4\\) y \\(z=1\\) está o no dentro del cascarón de los datos, en otras palabras, determine si se podría cometer extrapolación oculta al usar el modelo ajustado con \\(x=4\\) y \\(z=1\\). Punto atípico y punto influyente Los conceptos de atípico e influyente son diferentes y se definen así: Punto atípico (outlier): es una observación que es numéricamente distante del resto de los datos. Punto influyente: punto que tiene impacto en las estimativas del modelo. En la siguiente figura se ilustra la diferencia entre los conceptos de atípico e influyente. ¿Cómo se puede saber si un punto es influyente? Para saber si un punto es influyente debemos tener una métrica o medida para determinar si él es influyente, una medida podría ser la Distancia de Cook. Distancia de Cook Es una medida de cómo influye la observación \\(i\\)-ésima sobre la estimación de \\(\\boldsymbol{\\beta}\\) al ser retirada del conjunto de datos. Una distancia de Cook grande significa que una observación tiene un peso grande en la estimación de \\(\\boldsymbol{\\beta}\\). \\[ D_i = \\frac{\\sum_{j=1}^{n} (\\hat{y}_j - \\hat{y}_{j(i)} )^2 }{p \\hat{\\sigma^2}}, \\] donde la notación \\((i)\\) significa “sin la observación \\(i\\)-ésima”, eso quiere decir que \\(\\hat{y}_{j(i)}\\) es la estimación de \\(j\\)-ésima sin haber tenido en cuenta \\(i\\)-ésima observación en el ajuste del modelo. La cantidad \\(p\\) se refiere a todos los \\(\\beta\\)’s en el modelo (\\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k\\)). Son puntos influyentes las observaciones que presenten \\(D_i=\\frac{4}{n-p-2}\\). Ejemplo: ¿cómo se relaciona el peso corporal con la circunferencia de la muñeca, cuello y estatura? En este ejemplo se usará una base de datos que contiene medidad corporales para un grupo de estudiantes universitarios que vieron el curso de modelos de regresión en el año 2013. Abajo se muestra una figura ilustrativa de los datos. El objetivo es ajustar un modelo de regresión para explicar el peso promedio en función de la circunferencia de la muñeca, cuello y estatura. Luego de ajustar el modelo se deben identificar los posible estudiantes influyentes y el efecto de ellos en el modelos. Solución Lo primero es cargar los datos en nuestra sesión de R. url &lt;- &quot;https://raw.githubusercontent.com/fhernanb/datos/master/medidas_cuerpo2&quot; datos &lt;- read.table(file=url, sep=&quot;\\t&quot;, header=TRUE) head(datos, n=5) ## Peso Sexo Estatura circun_cuello circun_muneca ## 1 47.6 F 1.57 29.5 13.9 ## 2 68.1 M 1.66 38.4 16.0 ## 3 68.0 M 1.90 36.5 16.6 ## 4 80.0 M 1.76 38.0 17.1 ## 5 68.1 M 1.83 38.0 17.1 Antes de ajustar cualquier modelo es fundamental hacer un análisis descriptivo de los datos. Comenzaremos construyendo un diagrama de dispersión con pairs. panel.cor &lt;- function(x, y, digits=2, prefix=&quot;&quot;, cex.cor, ...) { usr &lt;- par(&quot;usr&quot;); on.exit(par(usr)) par(usr=c(0, 1, 0, 1)) r &lt;- cor(x, y) txt &lt;- format(c(r, 0.123456789), digits=digits)[1] txt &lt;- paste(prefix, txt, sep=&quot;&quot;) if(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt) text(0.5, 0.5, txt, cex=cex.cor * r, col=gray(1-r)) } # Creamos el grafico SOLO para las variables cuantitativas pairs(datos[, c(&quot;Peso&quot;, &quot;Estatura&quot;, &quot;circun_cuello&quot;, &quot;circun_muneca&quot;)], pch=19, las=1, upper.panel=panel.smooth, lower.panel=panel.cor) De la figura anterior se observa que hay un punto que se aleja de la nube, es un estudiante que pesa un poco más de 100 kilogramos. Vamos ahora a ajustar nuestro primer modelo. mod1 &lt;- lm(Peso ~ Estatura + circun_cuello + circun_muneca, data=datos) summary(mod1) ## ## Call: ## lm(formula = Peso ~ Estatura + circun_cuello + circun_muneca, ## data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.8842 -3.3779 -0.8704 2.5255 19.5692 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -62.0183 21.2383 -2.920 0.00793 ** ## Estatura 9.4404 20.0299 0.471 0.64206 ## circun_cuello 2.2587 0.8452 2.672 0.01391 * ## circun_muneca 1.9891 2.7458 0.724 0.47644 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.134 on 22 degrees of freedom ## Multiple R-squared: 0.7582, Adjusted R-squared: 0.7252 ## F-statistic: 22.99 on 3 and 22 DF, p-value: 5.637e-07 Como en la tabla anterior aparecen variables que nos son significativas vamos a realizar una selección de variables usando el paquete mixlm creado por Liland (2019). Vamos a realizar una selección de variables de manera que sólo queden variables significativas con un \\(\\alpha=0.04\\). mod2 &lt;- mixlm::backward(mod1, alpha=0.04) ## Backward elimination, alpha-to-remove: 0.04 ## ## Full model: Peso ~ Estatura + circun_cuello + circun_muneca ## &lt;environment: 0x00000000390759c8&gt; ## ## Step RSS AIC R2pred Cp F value Pr(&gt;F) ## Estatura 1 836.03 96.235 0.64981 2.2221 0.2221 0.6421 ## circun_muneca 2 886.31 95.753 0.67223 1.5587 1.3833 0.2516 summary(mod2) ## ## Call: ## lm(formula = Peso ~ circun_cuello, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.6933 -3.3996 -0.2729 2.6448 18.2076 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -44.6178 13.4939 -3.307 0.00296 ** ## circun_cuello 3.0990 0.3739 8.288 1.68e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## s: 6.077 on 24 degrees of freedom ## Multiple R-squared: 0.7411, ## Adjusted R-squared: 0.7303 ## F-statistic: 68.69 on 1 and 24 DF, p-value: 1.678e-08 En la siguiente tabla se comparan los modelos 1 y 2 ajustados hasta ahora. De la tabla anterior podemos destacar lo siguiente: El intercepto estimado cambia bastante. En el modelo 2 la variable cuello aumenta su efecto. El R2 se mantiene constante. La varianza de los errores disminuye, eso significa que el modelo 2 deja menos sin explicar. Vamos ahora a crear el diagrama de dispersión con el modelo ajustado. # Para construir el grafico de dispersion with(datos, plot(x=circun_cuello, y=Peso, pch=19, las=1, xlab=&quot;Circunferencia cuello (cm)&quot;, ylab=&quot;Peso (Kg)&quot;)) # Ahora agregamos la linea de tendencia abline(mod2, lwd=3, col=&#39;blue2&#39;) # por ultimo un texto con la ecuacion o modelo ajustado text(x=34, y=95, expression(hat(Peso) == -44.61 + 3.10 * C.cuello), col=&#39;blue3&#39; ) De la figura anterior vemos que hay un estudiante (el de 100 kilos de peso) que está muy alejado de la recta de regresión. Vamos a calcular las distancia de Cook para las observaciones del modelo 2 así: cooks.distance(mod2) ## 1 2 3 4 5 6 ## 2.597726e-03 3.718175e-02 1.437645e-04 3.872482e-02 2.098520e-02 2.691879e-03 ## 7 8 9 10 11 12 ## 4.858671e-03 2.777286e-01 1.098212e-03 2.062656e-03 9.764606e-01 7.749089e-02 ## 13 14 15 16 17 18 ## 7.199329e-02 1.118988e-03 1.743413e-02 8.336090e-04 2.922296e-02 6.289646e-04 ## 19 20 21 22 23 24 ## 3.650759e-02 1.223109e-02 1.940319e-02 2.854936e-02 7.612115e-03 3.328311e-02 ## 25 26 ## 3.585844e-03 1.457791e-06 Es mejor representar las distancias de Cook en forma gráfica para identificar los posible puntos influyentes así: cutoff &lt;- 4 / (26-2-2) plot(mod2, which=4, cook.levels=cutoff, las=1) abline(h=cutoff, lty=&quot;dashed&quot;, col=&quot;springgreen3&quot;) De esta figura es claro que las observaciones 11 y 8 tienen \\(D_i\\) por encima de la cota y se consideran observaciones influyentes. Ahora vamos a revisar los residuales del modelo 2. par(mfrow=c(2, 2)) plot(mod2, col=&#39;deepskyblue4&#39;) De la anterior figura vemos que las observaciones 8, 11 y 13 fueron identificadas por tener valores de residuales grandes. Vamos ahora a identificar las observaciones 8, 11, 12 y 13 en un diagrama de dispersión. La observación 11 es un hombre que pesa más de 100 kilos y que solo mide 1.79 metros. Las observaciones 8, 12 y 13 son mujeres con las mayores diferencias entre \\(y_i\\) y \\(\\hat{y}_i\\), para ellas el modelo sobreestima el peso corporal. En la siguiente tabla se muestran los resultados de ajustar nuevamente el modelo 2 bajo tres situaciones: con todas las observaciones, sin la observación 11 y sin las observaciones 8, 11, 12 y 13. De la tabla vemos que la observación 11 es muy influyente, al sacar esa observación el modelo aumenta su \\(R^2\\) y disminuye su \\(\\sigma^2\\). De la última columna se observa el mismo comportamiento, \\(R^2\\) aumenta y disminuye su \\(\\sigma^2\\) al sacar todas las observaciones sospechosas. Pero, ¿cuál modelo debo usar como modelo final? ¿El modelo 2, el modelo 2 sin la obs 11 o el modelo 2 sin las obs 8, 11, 12 y 13? Lo que se recomienda es que el analista se asesore de un experto en el área de aplicación para que juntos estudien esas observaciones sospechosas. Si hay una razón de peso para considerarlas como observaciones atípicas, ellas deben salir del modelo. Si por el contrario, no hay nada raro con las observaciones ellas deben seguir en el modelo. Las observaciones sospechosas NO se deben sacar inmediatamente del modelo. Antes se deben estudiar para ver si hay algo raro con ellas, en caso afirmativo se sacan de la base y se ajusta nuevamente el modelo. Una observación influyente NO es una observación mala en el modelo. Al contrario, ella es una observación clave en el ajuste porque “lidera” la estimación. Una observación que no es influyente es una observación que estando presente o no, el modelo ajustado no se ve afectado. DFFITS Se puede investigar la influencia de eliminar la \\(i\\)-ésima observación sobre el valor predicho o ajustado. \\[ DFFITS_i = \\frac{\\hat{y}_i - \\hat{y}_{i(i)}}{\\sqrt{s_{(i)}^2 h_{ii}}}. \\] Se sugiere que merece investigarse toda observación con \\(|DFFITS| &gt; 2 \\sqrt{p/n}\\). La función en R para obtener los \\(DFFITS_i\\) es dffits. DFBETAS Indica cuánto cambia el coeficiente de regresión, en unidades de desviación estándar, si se omitiera la 𝑖-ésima observación. \\[ DFBETAS_{i,j} = \\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}}{\\sqrt{s_{(i)}^2 C_{jj}}}, \\] donde \\(C_{jj}\\) es \\(j\\)-ésimo elemento de la diagonal de \\((\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\). Se sugiere que merece investigarse toda observación con \\(|DFBETAS| &gt; 2 \\sqrt{n}\\). La función en R para obtener los \\(DFBETAS_{i,j}\\) es dfbetas. Ejemplo En R se pueden encontrar las funciones dfbeta y dfbetas, esto puede generar alguna confusión y por esa razón vamos a explicar con detalle el asunto. Solución Vamos a crear unos datos para el ejemplo. # Creando unos datos de ejemplo datos &lt;- data.frame(x=1:5, y=rnorm(5)) mod &lt;- lm(y ~ x, data=datos) dfbeta(mod) ## (Intercept) x ## 1 -1.3221238 3.305310e-01 ## 2 0.5540403 -1.108081e-01 ## 3 0.1565594 2.748269e-17 ## 4 0.1335995 -1.335995e-01 ## 5 -0.1943646 9.718231e-02 dfbetas(mod) ## (Intercept) x ## 1 -1.5941063 1.321763e+00 ## 2 0.6133199 -4.068304e-01 ## 3 0.1550826 9.029002e-17 ## 4 0.1722504 -5.712900e-01 ## 5 -0.1747695 2.898225e-01 # DFBETAS para la pendiente manual para i=1 mod1 &lt;- lm(y ~ x, data=datos[-1, ]) # Para obtener dfbeta de pendiente para i=1 coef(mod)[2] - coef(mod1)[2] ## x ## 0.330531 # Para obtener dfbetas de pendiente para i=1 numerator &lt;- mod$coef[2] - mod1$coef[2] denominator &lt;- sqrt(summary(mod1)$sigma^2 * diag(summary(mod)$cov.unscaled)[2]) DFBETAS &lt;- numerator/denominator DFBETAS ## x ## 1.321763 En conclusión, La función dfbeta entrega \\(\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}\\). La función dfbetas entrega \\(\\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}}{\\sqrt{s_{(i)}^2 C_{jj}}}\\). ¿Cuál de las dos funciones se debe usar? References "]
]
